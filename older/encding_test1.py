# Generated by ChatGPT + Updated 

from transformers import GPT2Tokenizer, GPT2Model
import torch
from utils import printTokenizerDetails
# Initialize the GPT-2 tokenizer and model
tokenizer = GPT2Tokenizer.from_pretrained('gpt2')
model = GPT2Model.from_pretrained('gpt2')
tokenizer.pad_token = tokenizer.eos_token
#printTokenizerDetails(tokenizer) # model_max_length: 1024 # vocab_size: 50257

# Define the input text
input_text = "This is a long input text sequence that we want to encode with the GPT-2 tokenizer and model."
for i in range(0,10):
    input_text += input_text 

encoding = tokenizer(input_text, truncation=False, padding=True,return_tensors='pt')
print(len(input_text))
print(encoding.input_ids.shape)
print(encoding.attention_mask.shape)

# Note , if we give truncation as False then the token sequence length goes more than model_max_length
# Token indices sequence length is longer than the specified maximum sequence length for this
#  model (23552 > 1024). Running this sequence through the model will result in indexing errors
# 95232
# torch.Size([1, 23552])
# torch.Size([1, 23552])

# However we are not running through the model; We will add it to an array and train with block_size

block_size = tokenizer.model_max_length # 1024

import sys
sys.exit(0)

# If we need to run the model through a large encoding - see below

# Set the window size and stride
window_size = 512
stride = 256

# Split the input text into overlapping windows
windows = []
start = 0
while start < len(input_text):
    end = start + window_size
    if end >= len(input_text):
        end = len(input_text)
    windows.append(input_text[start:end])
    start += stride

# Encode each window separately and concatenate the embeddings
embeddings = []
for window in windows:
    encoding = tokenizer(window, truncation=True, padding=True,return_tensors='pt')
    with torch.no_grad():
        output = model(**encoding)
    embeddings.append(output.last_hidden_state[:, 0, :])
concatenated_embeddings = torch.cat(embeddings, axis=1)

# Print the shape of the concatenated embeddings
print(concatenated_embeddings.shape)